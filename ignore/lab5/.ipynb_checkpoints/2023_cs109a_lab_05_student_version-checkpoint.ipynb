{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \u003cimg style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"\u003e CS109A Introduction to Data Science \n",
                "\n",
                "## Lab 5: Regularization, Model Selection, and Confidence Intervals \n",
                "\n",
                "**Harvard University**\u003cbr/\u003e\n",
                "**Fall 2023**\u003cbr/\u003e\n",
                "**Instructors**: Pavlos Protopapas and Kevin Rader\u003cbr/\u003e\n",
                "\u003chr style='height:2px'\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Material preparation**: Robert Roessler, Queenie Luo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data and Stats packages\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "#pd.set_option('max_columns', 200)\n",
                "\n",
                "# Visualization packages\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "sns.set()\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.metrics import mean_squared_error\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn import preprocessing\n",
                "from sklearn.metrics import mean_squared_error\n",
                "from sklearn.metrics import r2_score\n",
                "from sklearn.model_selection import cross_validate\n",
                "#from prettytable import PrettyTable"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import RidgeCV\n",
                "from sklearn.linear_model import LassoCV\n",
                "from sklearn.linear_model import Ridge\n",
                "from sklearn.linear_model import Lasso\n",
                "import sys"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.rcParams.update({\n",
                "    \"text.usetex\": False,\n",
                "    \"font.family\": \"serif\",\n",
                "    \"font.sans-serif\": [\"Times New Roman\"],\n",
                "    \"figure.dpi\" : 100})\n",
                "\n",
                "# to display nice pipeline drawing\n",
                "from sklearn import set_config\n",
                "set_config(display=\"diagram\") # instead of display='text'\n",
                "pd.set_option('display.max_columns', None)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003ca id=top\u003e\u003c/a\u003e\n",
                "### Lab Overview\n",
                "\n",
                "- [1 - Cross Validation](#cv)\u003cBR\u003e\n",
                "- [2 - Bootstrapping](#boot)\u003cBR\u003e\n",
                "- [3 - Confidence Intervals](#ci)\u003cBR\u003e\n",
                "- [4 - Feature importance](#feature)\u003cBR\u003e\n",
                "- [5 - Linear Regression Coefficient interpretation](#coeff_interpretation)\u003cBR\u003e\n",
                "- [6 - Model Selection review with an example of ridge and lasso regression using cross validation.](#model)\u003cBR\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The dataset\n",
                "We are revisiting the dataset from last week's lab, which focused on Premier League Soccer Data. With this dataset, our primary objective was to predict the market value of Premier League players, denominated in millions of pounds, based on a variety of features.\n",
                "\n",
                "In the previous lab, you learned about the critical preparatory steps before model training. This involved feature engineering, where transformations such as `OneHotEncoder()` might be applied to handle categorical data. We also introduced the `PolynomialFeatures()` method, which allowed us to generate higher-degree features as well as interaction terms, capturing relationships between different features. Most importantly, we emphasized the importance of standardizing the features using techniques like the `StandardScaler()` to ensure that our model treats all features on a common scale. The data we're loading now is the outcome of those efforts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "\u003cdiv\u003e\n",
                            "\u003cstyle scoped\u003e\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "\u003c/style\u003e\n",
                            "\u003ctable border=\"1\" class=\"dataframe\"\u003e\n",
                            "  \u003cthead\u003e\n",
                            "    \u003ctr style=\"text-align: right;\"\u003e\n",
                            "      \u003cth\u003e\u003c/th\u003e\n",
                            "      \u003cth\u003epage_views\u003c/th\u003e\n",
                            "      \u003cth\u003efpl_points\u003c/th\u003e\n",
                            "      \u003cth\u003eage\u003c/th\u003e\n",
                            "      \u003cth\u003epage_views^2\u003c/th\u003e\n",
                            "      \u003cth\u003epage_views fpl_points\u003c/th\u003e\n",
                            "      \u003cth\u003epage_views age\u003c/th\u003e\n",
                            "      \u003cth\u003efpl_points^2\u003c/th\u003e\n",
                            "      \u003cth\u003efpl_points age\u003c/th\u003e\n",
                            "      \u003cth\u003eage^2\u003c/th\u003e\n",
                            "      \u003cth\u003eposition_cat_2\u003c/th\u003e\n",
                            "      \u003cth\u003eposition_cat_3\u003c/th\u003e\n",
                            "      \u003cth\u003eposition_cat_4\u003c/th\u003e\n",
                            "      \u003cth\u003enew_signing_1\u003c/th\u003e\n",
                            "      \u003cth\u003ebig_club_1\u003c/th\u003e\n",
                            "      \u003cth\u003eregion_2\u003c/th\u003e\n",
                            "      \u003cth\u003eregion_3\u003c/th\u003e\n",
                            "      \u003cth\u003eregion_4\u003c/th\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "  \u003c/thead\u003e\n",
                            "  \u003ctbody\u003e\n",
                            "    \u003ctr\u003e\n",
                            "      \u003cth\u003e0\u003c/th\u003e\n",
                            "      \u003ctd\u003e-0.378867\u003c/td\u003e\n",
                            "      \u003ctd\u003e-0.705517\u003c/td\u003e\n",
                            "      \u003ctd\u003e-0.689254\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.143541\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.267297\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.261136\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.497754\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.486280\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.475071\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "    \u003ctr\u003e\n",
                            "      \u003cth\u003e1\u003c/th\u003e\n",
                            "      \u003ctd\u003e-0.649806\u003c/td\u003e\n",
                            "      \u003ctd\u003e-0.782937\u003c/td\u003e\n",
                            "      \u003ctd\u003e-0.689254\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.422248\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.508758\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.447882\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.612991\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.539642\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.475071\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "    \u003ctr\u003e\n",
                            "      \u003cth\u003e2\u003c/th\u003e\n",
                            "      \u003ctd\u003e-0.090449\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.533214\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.074738\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.008181\u003c/td\u003e\n",
                            "      \u003ctd\u003e-0.048228\u003c/td\u003e\n",
                            "      \u003ctd\u003e-0.006760\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.284317\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.039852\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.005586\u003c/td\u003e\n",
                            "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "    \u003ctr\u003e\n",
                            "      \u003cth\u003e3\u003c/th\u003e\n",
                            "      \u003ctd\u003e0.558494\u003c/td\u003e\n",
                            "      \u003ctd\u003e1.636458\u003c/td\u003e\n",
                            "      \u003ctd\u003e-0.434590\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.311916\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.913952\u003c/td\u003e\n",
                            "      \u003ctd\u003e-0.242716\u003c/td\u003e\n",
                            "      \u003ctd\u003e2.677993\u003c/td\u003e\n",
                            "      \u003ctd\u003e-0.711188\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.188868\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "    \u003ctr\u003e\n",
                            "      \u003cth\u003e4\u003c/th\u003e\n",
                            "      \u003ctd\u003e0.271168\u003c/td\u003e\n",
                            "      \u003ctd\u003e2.352598\u003c/td\u003e\n",
                            "      \u003ctd\u003e-0.434590\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.073532\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.637948\u003c/td\u003e\n",
                            "      \u003ctd\u003e-0.117847\u003c/td\u003e\n",
                            "      \u003ctd\u003e5.534719\u003c/td\u003e\n",
                            "      \u003ctd\u003e-1.022415\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.188868\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e1.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "      \u003ctd\u003e0.0\u003c/td\u003e\n",
                            "    \u003c/tr\u003e\n",
                            "  \u003c/tbody\u003e\n",
                            "\u003c/table\u003e\n",
                            "\u003c/div\u003e"
                        ],
                        "text/plain": [
                            "   page_views  fpl_points       age  page_views^2  page_views fpl_points  \\\n",
                            "0   -0.378867   -0.705517 -0.689254      0.143541               0.267297   \n",
                            "1   -0.649806   -0.782937 -0.689254      0.422248               0.508758   \n",
                            "2   -0.090449    0.533214  0.074738      0.008181              -0.048228   \n",
                            "3    0.558494    1.636458 -0.434590      0.311916               0.913952   \n",
                            "4    0.271168    2.352598 -0.434590      0.073532               0.637948   \n",
                            "\n",
                            "   page_views age  fpl_points^2  fpl_points age     age^2  position_cat_2  \\\n",
                            "0        0.261136      0.497754        0.486280  0.475071             0.0   \n",
                            "1        0.447882      0.612991        0.539642  0.475071             0.0   \n",
                            "2       -0.006760      0.284317        0.039852  0.005586             1.0   \n",
                            "3       -0.242716      2.677993       -0.711188  0.188868             0.0   \n",
                            "4       -0.117847      5.534719       -1.022415  0.188868             0.0   \n",
                            "\n",
                            "   position_cat_3  position_cat_4  new_signing_1  big_club_1  region_2  \\\n",
                            "0             0.0             1.0            0.0         0.0       0.0   \n",
                            "1             1.0             0.0            1.0         0.0       1.0   \n",
                            "2             0.0             0.0            0.0         0.0       1.0   \n",
                            "3             0.0             1.0            0.0         1.0       1.0   \n",
                            "4             0.0             0.0            0.0         0.0       1.0   \n",
                            "\n",
                            "   region_3  region_4  \n",
                            "0       0.0       0.0  \n",
                            "1       0.0       0.0  \n",
                            "2       0.0       0.0  \n",
                            "3       0.0       0.0  \n",
                            "4       0.0       0.0  "
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Loading the data\n",
                "design_train_df = pd.read_csv(\"data/design_train_df.csv\")\n",
                "y_train = pd.read_csv(\"data/y_train.csv\")  # you could add .values.ravel() to make sure y_train is a 1D array\n",
                "\n",
                "design_test_df = pd.read_csv(\"data/design_test_df.csv\")\n",
                "y_test = pd.read_csv(\"data/y_test.csv\")\n",
                "\n",
                "display(design_train_df.head(5))\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=cv style=\"border: thin solid black; background: lightsalmon; padding: 5px\"\u003e\n",
                "\u003ch1\u003e1 - Cross Validation\u003c/h1\u003e \n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Intro\n",
                "\n",
                "Cross-validation is a technique for evaluating predictive models. In k-fold cross-validation, the dataset is divided into k subsets or folds. The model is trained and evaluated k times, using a different fold as the validation set each time. Performance metrics from each fold are averaged to estimate the model's generalization performance.\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cimg src=\"fig/cross_val.png\" alt=\"alt text\" width=\"500\" height=\"300\" class=\"blog-image\"\u003e\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "---\n",
                "\n",
                "### Key Points for Cross Validation\n",
                "\n",
                "\u003cdiv style=\"padding-top:0px;\"\u003e\u003c/div\u003e\n",
                "\n",
                "1. **Role of Training, Validation, and Test Sets:**\n",
                "    - **Key Takeaway:** The training set is used to fit candidate models, their performance on the validation set is used to select from among candidates, and the selected model's performance is finally evaluated on the test set.\n",
                "    - **Rationale:** Use of a validation set for model selection prevents overfitting to the training data. A separate test set helps us estimate the selected model's performance on previously unseen data.\n",
                "\n",
                "\u003cdiv style=\"padding-top:0px;\"\u003e\u003c/div\u003e\n",
                "\n",
                "2. **$k$-Fold Cross Validation:**\n",
                "    - **Key Takeaway:** One common method of model selection is $k$-Fold cross-validation where the original training data is partitioned into $k$ subsets or 'folds.' The model is trained on $k$-1 folds and validated on the remaining one. This process is repeated K times.\n",
                "    - **Rationale:** By rotating the validation set through all data points, we can get a more consistent measure of model performance.\n",
                "\n",
                "\u003cdiv style=\"padding-top:0px;\"\u003e\u003c/div\u003e\n",
                "\n",
                "3. **Choosing the Right Number of Folds:**\n",
                "    - **Key Takeaway:** The choice of $k$ in $k$-Fold cross-validation is crucial. Common choices are 5 or 10, but the optimal number may vary depending on the dataset size and specific problem.\n",
                "    - **Rationale:** A smaller $k$ will result in larger validation sets, which gives more accurate estimates of model performance but with higher variance. A larger $k$ provides smaller validation sets, leading to a less accurate model performance estimate but with lower variance.\n",
                "\n",
                "---\n",
                "\n",
                "### Manual Implementation of k-fold Cross-Validation"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"alert alert-success\"\u003e\n",
                "\u003ch3\u003eInteractive Coding Section\u003c/h3\u003e \n",
                "\n",
                "Think along and help me fill out the missing lines of code"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# K-Fold cross-validation\n",
                "\n",
                "# Number of partitions/folds to divide the dataset into\n",
                "k = 8\n",
                "\n",
                "# Calculate the size of each fold. If the dataset's length isn't \n",
                "# perfectly divisible by k, some folds might have an extra data point.\n",
                "fold_size = len(design_train_df) // k\n",
                "\n",
                "# This list will store the mean squared error for each fold.\n",
                "all_mse = []\n",
                "\n",
                "# Iterate over each fold\n",
                "for i in range(k):\n",
                "\n",
                "    # Compute the starting and ending indices for the test set based on the current fold.\n",
                "    # For example, if fold_size is 100:\n",
                "    # i=0 -\u003e start=0, end=100\n",
                "    # i=1 -\u003e start=100, end=200\n",
                "    # ... and so on\n",
                "    start, end = i * fold_size, (i + 1) * fold_size\n",
                "\n",
                "    print(start, end)\n",
                "    \n",
                "    # Use slicing to get the test set for the current fold from the main dataset.\n",
                "    #X_val_fold = design_train_df[...] # TODO\n",
                "    # y_val_fold = ... # TODO\n",
                "\n",
                "    # TODO: EXPLAIN WHAT'S GOING ON HERE!\n",
                "    X_train_fold = pd.concat([design_train_df[:start], design_train_df[end:]])\n",
                "    y_train_fold = np.concatenate([y_train[:start], y_train[end:]])\n",
                "\n",
                "    # Initialize a linear regression model\n",
                "    model = LinearRegression()\n",
                "    \n",
                "    # Train the model using the training set\n",
                "    # model.fit(..., ...) TODO\n",
                "    \n",
                "    # Predict the target values for the test set\n",
                "    predictions = model.predict(...) # TODO\n",
                "\n",
                "    # Compute the mean squared error for the current fold's predictions\n",
                "    mse = mean_squared_error(..., predictions) # TODO\n",
                "\n",
                "    # Store the computed MSE to our list\n",
                "    all_mse.append(mse)\n",
                "\n",
                "# Calculate the average mean squared error across all folds.\n",
                "avg_mse = np.mean(all_mse)\n",
                "\n",
                "# Print the result\n",
                "print(f\"Average MSE from simplified scratch implementation: {avg_mse:.2f}\")\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### K-fold Cross-Validation with scikit-learn's 'cross_validate()'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Using cross_validate function on training data\n",
                "model = LinearRegression()\n",
                "scores = cross_validate(model, design_train_df, y_train, cv=8, scoring='neg_mean_squared_error') # negative MSE because it's a loss function\n",
                "\n",
                "avg_mse_sklearn = -np.mean(scores['test_score'])  # Convert negative MSE back to positive\n",
                "print(f\"Average MSE using scikit-learn: {avg_mse_sklearn:.2f}\")\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "### You might be wondering:\n",
                "\n",
                "**\"why neg_mean_squared_error\"?**\n",
                "  \n",
                "*Answer:* In Scikit-learn, a general principle is that greater values should always correspond to better outcomes. This makes sense when thinking about scores like accuracy or R^2 where higher values are obviously better. However, when it comes to loss functions like Mean Squared Error (MSE), a lower value is better. So, to make it fit into Scikit-learn's general principle, the negative of the MSE is used. This way, larger values (which are less negative) still indicate better models. This principle is embedded in Scikit-learn's unified scoring API. \n",
                "When you perform hyperparameter tuning using something like GridSearchCV or RandomizedSearchCV, Scikit-learn tries to maximize the score. If you used a positive MSE, the optimization routine would erroneously try to maximize the MSE, which isn't what we want. By using a negative MSE, the optimization correctly tries to find a model that gets the \"maximum\" score, which corresponds to the minimum actual MSE.\n",
                "\n",
                "**\"Why use K-Fold and not e.g. LPOCV or LOOCV (Leave-P-Out or Leave-One-Out Cross Validation)?\"**\n",
                "\n",
                "*Answer:* LOOCV can be computationally expensive for large datasets, as it involves training a model n times (where n is the number of data points). K-Fold, especially with k=5 or 10, provides a good balance between computational cost and reliable estimation of model performance.\n",
                "\n",
                "**In the manual cross-validation example, why didn't we shuffle the data?**\n",
                "\n",
                "*Answer:* The manual example was a simplified demonstration. In many real-world scenarios, especially if the data has some order to it, you'd want to shuffle the data or use StratifiedKFold to maintain class distributions."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=boot style=\"border: thin solid black; background: lightsalmon; padding: 5px\"\u003e\n",
                "\u003ch1\u003e2 - Bootstrapping\u003c/h1\u003e \n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cimg src=\"fig/bootstrap_example_rader.jpeg\" alt=\"alt text\" width=\"500\" height=\"300\" class=\"blog-image\"\u003e\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "### Key Points for Bootstrapping:\n",
                "\n",
                "1. **Sample Size Consistency:** \n",
                "    - **Key Takeaway:** The bootstrap dataset should have the same number of points as the original.\n",
                "    - **Rationale:** This preserves the structure and variability of the original dataset in the bootstrap samples, ensuring that we are drawing from a distribution similar to our original data.\n",
                "\n",
                "\u003cdiv style=\"padding-top:0px;\"\u003e\u003c/div\u003e\n",
                "\n",
                "2. **Sampling with Replacement:** \n",
                "    - **Key Takeaway:** We must sample with replacement; each time an item is selected, we put it back in the original dataset, so it may possibly be reselected.\n",
                "    - **Rationale:** By sampling with replacement, each bootstrap sample is an independent draw from the original dataset. This captures the idea that we are trying to emulate the process of obtaining new samples from the population.\n",
                "\n",
                "\u003cdiv style=\"padding-top:0px;\"\u003e\u003c/div\u003e\n",
                "\n",
                "3. **Extensive Iterative Sampling for Robust Bootstrapping Results:**\n",
                "    - **Key Takeaway:** Typically, thousands of bootstrap samples are drawn (e.g., 10,000 or more) to obtain a reliable estimate of the distribution.\n",
                "    - **Rationale:** The more replicates we draw, the better our approximation to the true distribution of the statistic. This ensures that our estimates, such as confidence intervals, are robust and reliable.\n",
                "\n",
                "\u003cdiv style=\"padding-top:0px;\"\u003e\u003c/div\u003e\n",
                "\n",
                "4. **Limitations and Assumptions of Bootstrapping:**\n",
                "    - **Key Takeaway:** While bootstrapping is versatile and powerful, it's not always the best approach for every situation.\n",
                "    - **Rationale:** The bootstrap method assumes that the sample is a good representation of the population. If the original sample has biases or is not representative, the bootstrap samples will inherit these issues. \n",
                "\n",
                "\u003cdiv style=\"padding-top:0px;\"\u003e\u003c/div\u003e\n",
                "\n",
                "5. **Bootstrap and Confidence Intervals:**\n",
                "    - **Key Takeaway:** One common use of bootstrapping is to construct confidence intervals, which provide a range of values that likely contain the true parameter value.\n",
                "    - **Rationale:** Bootstrapped confidence intervals give a non-parametric way to estimate the range of possible values for a statistic without making strong assumptions about the shape or parameters of the true population distribution.\n",
                "\n",
                "---"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Bootstrap Implementation \n",
                "#### The laborious, but insightful way:\n",
                "\n",
                "1. Choose random data points by randomly choosing indices.\n",
                "2. Create subsets of the original data by choosing the DataFrame elements with the randomly chosen indices.\n",
                "\n",
                "This could be achieved by using numpy's `random.choice()` pandas' `.iloc[]` method."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create example df  \n",
                "example_X_df = design_train_df.head(5)\n",
                "example_y_df = y_train.head(5)\n",
                "\n",
                "# Generate random indices from example df\n",
                "boot_i = np.random.choice(example_X_df.index, replace = True, size = len(example_X_df.index)) \n",
                "\n",
                "# Generate an X_train_boot data frame that contains exactly the observations with the boot_i indices.\n",
                "# Print out the results of each line to understand what's going on. \n",
                "# Make sure that you understand why X_train_boot can contain the same index multiple times\n",
                "X_train_boot = example_X_df.iloc[boot_i]\n",
                "y_train_boot = example_y_df.iloc[boot_i]\n",
                "\n",
                "# display(X_train_boot)\n",
                "# display(y_train_boot)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### The fast way (if your training data wasn't split into X and y):\n",
                "\n",
                "1. Instead of calculating indices manually and using .iloc to grab the respective rows, you could also use pandas' built-in `.sample()` method\n",
                "2. However, since you don't have the indices to also grab the respective y_train_boot values in this case, you'd have to combine example_df and y_train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create an example dataframe\n",
                "example_X_df = design_train_df.head(5)\n",
                "\n",
                "# Take the corresponding rows from y_train and add it to the example_df\n",
                "example_X_y_df = example_X_df.copy()\n",
                "example_X_y_df['target'] = y_train.head(5)\n",
                "\n",
                "# Bootstrapping using pandas' sample method\n",
                "boot_df = example_X_y_df.sample(frac=1, replace=True)\n",
                "\n",
                "# Split the bootstrapped dataframe back into X_train_boot and y_train_boot\n",
                "X_train_boot = boot_df.drop(columns='target')\n",
                "y_train_boot = boot_df['target']\n",
                "\n",
                "# display(X_train_boot)\n",
                "# display(y_train_boot)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"alert alert-success\"\u003e\n",
                "\u003ch3\u003eYour turn! (10 mins)\u003c/h3\u003e \n",
                "\n",
                "Complete the code inside the loop below. Make sure to create a new bootstrap sample in every iteration, fit a linear regression model. Store the model in `boot_models` as well as the coefficients."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure number of bootstraps\n",
                "n_boots = 1000\n",
                "\n",
                "# Lists to save models and coefficients\n",
                "boot_models = []\n",
                "boot_betas = []\n",
                "\n",
                "# Decision to add intercept in the linear regression model\n",
                "fit_intercept = True\n",
                "\n",
                "for i in range(n_boots):\n",
                "   \n",
                "    # Randomly sample indices with replacement to create bootstrap samples\n",
                "    boot_i = ... # TODO\n",
                "\n",
                "    # Create bootstrap datasets for features and target variable using the sampled indices\n",
                "    X_train_boot = ... # TODO\n",
                "    y_train_boot = ... # TODO\n",
                "    \n",
                "    # Train a linear regression model on the bootstrap sample\n",
                "    boot_linreg = LinearRegression(fit_intercept = fit_intercept).fit(..., ...) # TODO\n",
                "\n",
                "    # Save the trained model\n",
                "    boot_models.append(...) # TODO\n",
                "    \n",
                "    # EXPLAIN AND COMMENT THE FOLLOWING NEXT TWO LINES\n",
                "    coefs = np.insert(boot_linreg.coef_[0], 0, boot_linreg.intercept_[0], axis=None)\n",
                "    boot_betas.append(coefs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# store betas in df\n",
                "boot_betas_df = pd.DataFrame(boot_betas) "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"alert alert-success\"\u003e\n",
                "\u003ch3\u003eNext Steps\u003c/h3\u003e \n",
                "\n",
                "if you have some time left, look into the following next steps. We will do this interactively in class.\n",
                "\n",
                "1. **Examine the Coefficients:** Take a closer look at the boot_betas to understand the distribution and variability of our bootstrapped coefficients.\n",
                "\n",
                "2. **Comparing Columns:** Determine the number of columns in boot_betas \u0026 the number of columns in design_train_df.\n",
                "\n",
                "\u003cdiv style='padding:0px'\u003e\u003c/div\u003e\n",
                "\n",
                "3. **Investigate Discrepancies:** Can you explain the difference in the number of columns between boot_betas and design_train_df?\n",
                "  \n",
                "\u003cdiv style='padding:0px'\u003e\u003c/div\u003e\n",
                "\n",
                "4. **Action Plan:** What steps do you think we need to take next, based on the results and discrepancies you've observed? List them in order."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Explore"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# display(boot_betas_df.head())\n",
                "# print(len(boot_betas_df.columns)) \n",
                "# display(design_train_df.columns)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Task \n",
                "Recover the feature names for each beta coefficient and add them to the data frame. Don't forget the intercept!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extracting the feature names from the design_train_df\n",
                "feature_names = ... # TODO\n",
                "\n",
                "# Adding 'x0' to the beginning of the list, representing the intercept in our linear model\n",
                "feature_names_final = ... # TODO\n",
                "\n",
                "# Creating a DataFrame to store the coefficients from all bootstrap models\n",
                "# Each row in boot_betas_df represents the coefficients (including the intercept) from one bootstrap model\n",
                "boot_betas_df = pd.DataFrame(..., columns=...) # TODO\n",
                "\n",
                "boot_betas_df"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=ci style=\"border: thin solid black; background: lightsalmon; padding: 5px\"\u003e\n",
                "\u003ch1\u003e3 - Confidence Intervals\u003c/h1\u003e \n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "After bootstrapping and obtaining a distribution of coefficients for each feature, a logical next step is to compute confidence intervals. Confidence intervals provide a range of values, derived from the bootstrapped data, in which we expect the true parameter (in this case, the coefficient) to lie, with a certain level of confidence.\n",
                "\n",
                "**Why is this useful?**\n",
                "\n",
                "- **Interpretability**: Confidence intervals help us understand the uncertainty and variability associated with our estimates. A narrower confidence interval indicates a more reliable estimate, while a wider interval suggests more uncertainty.\n",
                "  \n",
                "- **Statistical Significance**: By examining whether a confidence interval for a coefficient includes zero, we can make inferences about the statistical significance of that coefficient. If zero isn't in the interval, we might infer the corresponding feature has a significant association with the target.\n",
                "\n",
                "- **Comparisons**: By comparing the confidence intervals of different coefficients, we can gauge which features have more influence or certainty associated with their effects.\n",
                "\n",
                "In the following code, we will compute the 95% confidence intervals for each feature's coefficient. This means that we are 95% confident that the true coefficient lies within the provided range.\n",
                "\n",
                "---\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"alert alert-success\"\u003e\n",
                "\u003ch3\u003eTASK: Find the 95% confidence intervals for the coefficients\u003c/h3\u003e "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize an empty list to store statistics for each feature\n",
                "stats_list = []\n",
                "\n",
                "# Loop through each feature to compute its bootstrapped statistics\n",
                "for i in range(len(feature_names_final)):\n",
                "    \n",
                "    # Extracting the bootstrapped coefficient values for the current feature\n",
                "    betavals = boot_betas_df.iloc[:, i]\n",
                "    \n",
                "    # Sorting the coefficient values to aid in percentile calculation\n",
                "    betavals.values.sort()\n",
                "    \n",
                "    # TODO: COMPLETE AND EXPLAIN WHAT'S HAPPENING IN THE FOLLOWING TWO LINES\n",
                "    x1 = np.round(np.percentile(betavals, ...), 2)\n",
                "    x2 = np.round(np.percentile(betavals, ...), 2)\n",
                "    \n",
                "    # Calculating mean and standard deviation of the bootstrapped coefficients\n",
                "    mean = np.round(np.mean(betavals),2)\n",
                "    std = np.round(np.std(betavals),2)\n",
                "    \n",
                "    # Appending computed statistics for current feature to the stats_list\n",
                "    stats_list.append([feature_names_final[i], mean, std, x1, x2])\n",
                "\n",
                "# Convert the stats_list into a dataframe for easy visualization and analysis\n",
                "boot_beta_df = pd.DataFrame(stats_list, columns=['feature', 'boot_mean', 'boot_std', '95_low', '95_high'])\n",
                "\n",
                "# Display the final dataframe with bootstrapped statistics and confidence intervals\n",
                "boot_beta_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setting up the figure and axes\n",
                "fig, axs = plt.subplots(9, 2, figsize=(10, 40))\n",
                "axs = axs.ravel()\n",
                "\n",
                "# Looping through each feature to plot histograms and confidence intervals\n",
                "# Starting from 1 to exclude the intercept (assuming x0 is the first column)\n",
                "for i, (index, row) in enumerate(boot_beta_df.iloc[1:].iterrows()):\n",
                "    sns.histplot(boot_betas_df.iloc[:, i+1], bins = 20, ax=axs[i], kde=True, color='skyblue', edgecolor='black')\n",
                "    axs[i].axvline(row['95_low'], color='red', linestyle='--', label='2.5% Percentile')\n",
                "    axs[i].axvline(row['95_high'], color='green', linestyle='--', label='97.5% Percentile')\n",
                "    axs[i].axvline(row['boot_mean'], color='blue', linestyle='-', label='Mean')\n",
                "    axs[i].set_title(row['feature'])\n",
                "    axs[i].legend()\n",
                "\n",
                "# Adjusting layout\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Now let's plot the confidence intervals for a couple of the feature coefficients"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=feature style=\"border: thin solid black; background: lightsalmon; padding: 5px\"\u003e\n",
                "\u003ch1\u003e4 - Feature importance\u003c/h1\u003e "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's see how important our vaues are from the bootstrap values in DataFrame `boot_beta_df`. Sort the values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "sorted_boot_beta_df = boot_beta_df.sort_values(by=['boot_mean'], ascending=True)\n",
                "sorted_boot_beta_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots()\n",
                "ax.barh(sorted_boot_beta_df['feature'].iloc[:-1], sorted_boot_beta_df['boot_mean'].iloc[:-1], \n",
                "        align='center', color=\"#336600\", alpha=0.7)\n",
                "ax.grid(linewidth=0.2)\n",
                "ax.set_xlabel('Coefficient')\n",
                "ax.set_ylabel('Predictors')\n",
                "ax.set_title('Feature Importance based on the absolute value \\n of the coefficients over multiple bootstraps')\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Now let's find the feature importance using the t-values "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# creating a new column with the t-values  \n",
                "sorted_boot_beta_df['t'] = sorted_boot_beta_df.apply(lambda row: \n",
                "                                         row['boot_mean']/row['boot_std'], axis=1)\n",
                "sorted_boot_beta_df.head(2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots()\n",
                "ax.barh(sorted_boot_beta_df['feature'].iloc[:-1], sorted_boot_beta_df['t'].iloc[:-1], \n",
                "        align='center', color=\"#336600\", alpha=0.7)\n",
                "ax.grid(linewidth=0.2)\n",
                "ax.set_xlabel('Coefficient')\n",
                "ax.set_ylabel('Predictors')\n",
                "ax.set_title('Feature Importance based on t-test')\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=coeff_interpretation style=\"border: thin solid black; background: lightsalmon; padding: 5px\"\u003e\n",
                "\u003ch1\u003e5 - Linear Regression Coefficient Interpretation\u003c/h1\u003e "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Say we have input features $X$, which via some function $f()$, approximates outputs $Y$. That is, $Y = f(X) + \\epsilon$.\n",
                "\n",
                "- **Inference**: estimates the function $f$, and is more concerned with understanding the relationship between $X$ and $Y$.\n",
                "- **Prediction**: the goal is making accurate $Y$ predictions for some unseen $X$.\n",
                "\n",
                "\n",
                "#### Two popular and useful libraries in Python are `sklearn` and `statsmodels`.\n",
                "\n",
                "`statsmodels` is mostly focused on the _inference_ task. It aims to make good estimates for $f()$ (via solving for our $\\beta$'s), and it provides expansive details about its certainty. It provides lots of tools to discuss confidence, but isn't great at dealing with test sets.\n",
                "\n",
                "`sklearn` is mostly focused on the _prediction_ task. It aims to make a well-fit line to our input data $X$, so as to make good $Y$ predictions for some unseen inputs $X$. It provides a shallower analysis of our variables. In other words, `sklearn` is great at test sets and validations, but it can't really discuss uncertainty in the parameters or predictions."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Let's try `statsmodels` now.\n",
                "\n",
                "`statsmodels` linear regression does not include an intercept by default, so, if needed, we add one using:\n",
                "```\n",
                "Xtrain = sm.add_constant(Xtrain)\n",
                "sm.OLS(y,Xtrain)\n",
                "```\n",
                "where Xtrain is our train set and y our return variable. \n",
                "\n",
                "Watch for additional intercepts coming from, e.g. Polynomial features. Have only one intercept. In our case we do have one, so we are not adding another one. "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Let's load a clean set of data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "design_train_df = pd.read_csv(\"data/design_train_df.csv\")\n",
                "y_train = pd.read_csv(\"data/y_train.csv\")\n",
                "\n",
                "design_test_df = pd.read_csv(\"data/design_test_df.csv\")\n",
                "y_test = pd.read_csv(\"data/y_test.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import statsmodels.api as sm\n",
                "from statsmodels.api import OLS\n",
                "\n",
                "# TODO: fill in the input variable for add_constant()\n",
                "design_train_df = sm.add_constant(...)\n",
                "ols = OLS(y_train, np.array(design_train_df))\n",
                "results = ols.fit()\n",
                "# get the parameters\n",
                "results.params "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Can we recover the feature names for readability?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "feature_names = list(design_train_df.columns)\n",
                "feature_names"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_df = pd.DataFrame(zip(feature_names, results.params), columns=['feature', 'coeff'])\n",
                "results_df"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It seems like they are almost the same as the ones produced by using `sklearn` (previous lab), so that is a good sanity check!\n",
                "\n",
                "With `statsmodels` we can print more statistics than we need!\n",
                "```\n",
                "results.summary()\n",
                "```\n",
                "If we want human-readable feature names we need to set the parameter `xname`\n",
                "```\n",
                "results.summary(xname = a list of names)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "results.summary(xname=feature_names)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"alert alert-success\"\u003e\n",
                "    \u003cstrong\u003e 💬 DISCUSSION:\u003c/strong\u003e How do the bootstrap confidence intervals compare with the ones derived by `statsmodels` which are based on the Student’s t-distribution? \u003c/div\u003e "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"alert alert-success\"\u003e\n",
                "    \u003cstrong\u003e 💬 DISCUSSION:\u003c/strong\u003e What conclusions can we make from the values of the model coefficients as to the relationship between our response variable y and the features? \u003c/div\u003e \n",
                "\n",
                "```\n",
                "y = 8.1 + 4.7 x page_views + 2.9 x fpl_points - 1.3 x age - 0.21 x page_views^2 + 1.1 x page_views fpl_points - 1.3 x page_views age + 0.79 x fpl_points^2 - 0.75 x fpl_points age - 1.8 x age^2 + 1.1 x position_cat_2 + 0.075 x position_cat_3 + 0.25 x position_cat_4 + 1.9 x new_signing_1 + 6.9 x big_club_1 + 0.99 x region_2 + 1.5 x region_3 + 1.0 x region_4\n",
                "```\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=model style=\"border: thin solid black; background: lightsalmon; padding: 5px\"\u003e\n",
                "\u003ch1\u003e6 - Model Selection (review) with an example of ridge and lasso regression.\u003c/h1\u003e "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We are interested in the generalization performance of a learning method which shows as the prediction capability on an independent test data. This performance guides our choice of learning model, and gives us a measure of the quality of the ultimately chosen model. If we are in a data-rich situation, the best approach for both problems is to randomly divide the dataset into three parts: a training set, a validation set, and a test set. The training set is used to fit the models; the validation set is used to estimate prediction error for model selection; the test set is used for assessment of the generalization error of the final chosen model.\n",
                "\n",
                "- **Model selection**: estimating the performance of different models in order to choose the best one.\n",
                "- **Model assessment**: having chosen a final model, estimating its predic- tion error (generalization error) on new data.\n",
                "\u003cp style=\"text-align:right\"\u003e\u003cfont size=\"1\"; text-align='right'\u003e(The Elements of Statistical Learning (2009)\u003c/font\u003e\u003c/p\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Start with an original dataset "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![dataset](fig/data1.jpg) "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Split the original dataset and put the final test set away somewhere."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![valset](fig/data2.jpg) "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Create an additional validation set out of the train set\n",
                "\n",
                "Why do we need a validation set? Why not just assess the model by training with different hyperparameters?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![crossval](fig/data3.jpg) "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Using cross validation"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![jeffrey](fig/data4.jpg) "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Ridge regression using a validation set. "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Let's load a clean set of data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "design_train_df = pd.read_csv(\"data/design_train_df.csv\")\n",
                "y_train = pd.read_csv(\"data/y_train.csv\")\n",
                "\n",
                "design_test_df = pd.read_csv(\"data/design_test_df.csv\")\n",
                "y_test = pd.read_csv(\"data/y_test.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "design_train_df.head(2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# remove\n",
                "# Split the data into train and validation sets \n",
                "X_train, X_val, Y_train, Y_val = train_test_split(design_train_df, \n",
                "                                                  y_train, \n",
                "                                                  train_size=0.8, \n",
                "                                                  random_state=42\n",
                "                                                  )\n",
                "\n",
                "X_train.shape, X_val.shape, y_train.shape, Y_val.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# remove\n",
                "# List of hyper-parameter values \n",
                "alphas = np.logspace(0, 1, 100)\n",
                "\n",
                "# List to store training errors\n",
                "training_error = []\n",
                "# List to store validation errors\n",
                "validation_error = []\n",
                "\n",
                "fit_intercept = True\n",
                "\n",
                "for i, alpha in enumerate(alphas):\n",
                "    \n",
                "    # For each i, fit a ridge regression on training set\n",
                "    ridge_reg = Ridge(fit_intercept=..., # TODO: define your bool varible here\n",
                "                      alpha=..., # TODO: define your alpha here\n",
                "                      #max_iter=max_iter\n",
                "                     )\n",
                "    ridge_reg.fit(..., ...) # TODO: fit your model\n",
                "\n",
                "    # Predict on the train and validation set \n",
                "    Y_train_pred = ridge_reg.predict(X_train)\n",
                "    Y_val_pred = ridge_reg.predict(X_val)\n",
                "    \n",
                "    # Compute the training and validation MSE\n",
                "    mse_train = mean_squared_error(Y_train, Y_train_pred) \n",
                "    mse_val = mean_squared_error(Y_val, Y_val_pred)\n",
                "    \n",
                "    # Add that value to the list \n",
                "    training_error.append(mse_train) \n",
                "    validation_error.append(mse_val) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# remove\n",
                "# TODO: find the best parameter\n",
                "# STEPS: \n",
                "# 1. Finds the minimum value in the validation_error list.\n",
                "# 2. Finds the index of the smallest validation error in the validation_error list. \n",
                "# 3. Uses the index found in step 2 to retrieve the corresponding alpha value from the alphas list.\n",
                "best_alpha = ...\n",
                "print(best_alpha)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(6,4))\n",
                "\n",
                "ax.plot(alphas, training_error, label='training error');\n",
                "ax.plot(alphas, validation_error, label='validation error');\n",
                "\n",
                "ax.set_xlabel('Ridge tuning parameter (alpha)', fontsize=10)\n",
                "ax.set_ylabel('mse', fontsize=10)\n",
                "\n",
                "ax.set_title('Ridge Regression mse in train and test set', fontsize=12)\n",
                "ax.grid(\":\", alpha=0.4)\n",
                "\n",
                "ax.legend(loc='best')\n",
                "plt.tight_layout()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Let's re-fit the model on the whole dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# remove\n",
                "# We should use all the dataset now, training+validation\n",
                "# TODO: re-fit the model on the whole dataset using the best alpha\n",
                "final_ridge_model = Ridge(alpha=...).fit(design_train_df, y_train)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Let's do the final assessment on the test set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# remove\n",
                "y_test_pred = final_ridge_model.predict(design_test_df)\n",
                "r2_test = final_ridge_model.score(design_test_df, y_test)\n",
                "print(f'R^2 test = {r2_test:.3}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Lasso regression using cross validation. \n",
                "\u003cdiv class=\"alert alert-success\"\u003e\n",
                "\u003ch3\u003eYour turn! (5 mins)\u003c/h3\u003e \n",
                "\n",
                "Complete the code inside the loop below. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# remove\n",
                "# Set parameters for cross-validation\n",
                "alphas = np.logspace(0, 1, 100)\n",
                "fit_intercept = True\n",
                "# Maximum number of iterations to run \n",
                "# before it converges.\n",
                "max_iter = 1000\n",
                "\n",
                "lasso_coefficients = []\n",
                "k = 5\n",
                "num_est = 0\n",
                "# store MSE results\n",
                "results = []\n",
                "\n",
                "training_error = []\n",
                "validation_error = []\n",
                "validation_std = []\n",
                "\n",
                "for alpha in alphas:\n",
                "    # TODO:\n",
                "    # Initialize a Lasso regression model.\n",
                "    # Hint: Lasso(fit_intercept=..., alpha=...)\n",
                "    lasso_reg = ...\n",
                "    \n",
                "    # Perform k-fold cross-validation using the Lasso regression model.\n",
                "    # Hint: cross_validate(..., scoring=\"neg_mean_squared_error\", \n",
                "    #                           return_train_score=True, \n",
                "    #                           return_estimator=True)\n",
                "    lasso_cv = ...\n",
                "    \n",
                "    training_error.append(                       \n",
                "        np.mean(-lasso_cv['train_score'])\n",
                "    )\n",
                "    \n",
                "    validation_error.append(\n",
                "        np.mean(-lasso_cv['test_score'])\n",
                "    )\n",
                "    \n",
                "    validation_std.append(\n",
                "        np.std(lasso_cv['test_score'])\n",
                "    )\n",
                "    \n",
                "    lasso_coefficients.append(\n",
                "         lasso_cv['estimator']\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# find the best parameter\n",
                "min_cross_val_mse = min(validation_error)\n",
                "best_cross_val_alpha = alphas[validation_error.index(min_cross_val_mse)]\n",
                "min_cross_val_mse, best_cross_val_alpha"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(6,4))\n",
                "\n",
                "ax.plot(alphas, training_error, label='training error');\n",
                "ax.plot(alphas, validation_error, label='validation error');\n",
                "\n",
                "ax.set_xlabel('Lasso tuning parameter (alpha)', fontsize=10)\n",
                "ax.set_ylabel('mse', fontsize=10)\n",
                "\n",
                "ax.set_title('Lasso Regression mse in train and validation set', fontsize=12)\n",
                "ax.grid(\":\", alpha=0.4)\n",
                "\n",
                "ax.legend(loc='best')\n",
                "plt.tight_layout()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Let's re-fit the model on the whole dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We should use all the dataset now, training+validation\n",
                "\n",
                "# TODO: complete the code below\n",
                "cross_ridge_model = Ridge(alpha=...).fit(design_train_df, y_train)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Evaluate on the test set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# remove\n",
                "y_test_pred = cross_ridge_model.predict(design_test_df)\n",
                "r2_test = cross_ridge_model.score(design_test_df, y_test)\n",
                "print(f'R^2 test = {r2_test:.3}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Ridge and lasso regression using built-in cross validation in `sklearn`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "design_train_df = pd.read_csv(\"data/design_train_df.csv\")\n",
                "y_train = pd.read_csv(\"data/y_train.csv\")\n",
                "\n",
                "design_test_df = pd.read_csv(\"data/design_test_df.csv\")\n",
                "y_test = pd.read_csv(\"data/y_test.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "design_train_df.head(2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set parameters for cross-validation\n",
                "alphas = np.logspace(0, 1, 10)\n",
                "\n",
                "fit_intercept = True\n",
                "max_iter = 100000\n",
                "\n",
                "ridge_coefficients = []\n",
                "lasso_coefficients = []\n",
                "k = 10\n",
                "\n",
                "ridge = RidgeCV(alphas=alphas, cv=k).fit(design_train_df, y_train)\n",
                "lasso = LassoCV(alphas=alphas, cv=k).fit(design_train_df, y_train)\n",
                "\n",
                "ridge_a = ridge.alpha_\n",
                "print('Best alpha for ridge: {}'.format(ridge_a))\n",
                "print(f'R^2 score for Ridge with alpha={ridge_a}: {ridge.score(design_test_df, y_test):.3}')\n",
                "\n",
                "lasso_a = lasso.alpha_\n",
                "print('Best alpha for lasso: {}'.format(lasso_a))\n",
                "print(f'R^2 score for Lasso with alpha={lasso_a}: {lasso.score(design_test_df, y_test):.3}')"
            ]
        }
    ]
}
