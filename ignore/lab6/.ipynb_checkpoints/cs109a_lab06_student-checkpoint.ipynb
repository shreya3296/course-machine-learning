{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.ticker as mtick\n",
                "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
                "import seaborn as sns\n",
                "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
                "from sklearn.manifold import TSNE\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score\n",
                "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, RidgeCV\n",
                "from sklearn.metrics import mean_squared_error\n",
                "from sklearn.pipeline import make_pipeline, Pipeline\n",
                "from sklearn.base import BaseEstimator, TransformerMixin\n",
                "from sklearn.manifold import TSNE\n",
                "\n",
                "from warnings import simplefilter\n",
                "simplefilter('ignore', category=UserWarning) # ignore a pesky warning in fashion plot"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \u003cimg style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"\u003e CS109A Introduction to Data Science "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Lab 6: Principal Components Analysis (PCA)\n",
                "\n",
                "**Harvard University**\u003cbr/\u003e\n",
                "**Fall 2023**\u003cbr/\u003e\n",
                "**Instructors**: Pavlos Protopapas and Kevin Rader\u003cbr/\u003e\n",
                "\u003chr style='height:2px'\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Topics\n",
                "- Finding Principle Components\n",
                "    1. eigenvector decomposition\n",
                "    2. singular value decomposition (SVD)\n",
                "    3. SKLearn's PCA class\n",
                "- Projecting the Data onto the Principle Components\n",
                "- Inspecting Predictors' Contributions to Each Component\n",
                "- Principle Component Regression (PCR)\n",
                "    - Coefficients in terms of the original predictors\n",
                "    - Selecting the number of components\n",
                "    - PCA /w cross-validation \u0026 regularization\n",
                "- More PCA visualization examples\n",
                "- t-SNE: Non-linear dimensionality reduction"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test Scores Data"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For our first example, we'll use a data set of test scores in different courses for 1,000 students.\n",
                "\n",
                "The dataset is in `data/exams.csv`"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Read in the csv into a DataFrame and take a look with `df.head()`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv('data/exams.csv')\n",
                "df.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Define `X` to be only the course score columns, ignoring `Total`, `Results`, and `Div`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = df[['Hindi', 'English', 'Science', 'Maths', 'History', 'Geography']]\n",
                "X"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3 Ways to find the Principle Components"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### (1) Eigenvectors of the predictor covariance matrix"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The eigenvectors of the predictor covariance matrix, ordered by decreasing eigenvalues, correspond to orthogonal vectors in the predictor space ordered by decreasing amount of explained variance. Refer to the PCA advanced section for more details.\n",
                "\n",
                "Quick recap on **eigenvectors**. The eigenvectors of a matrix $\\bf{A}$ are the vectors that, when multiplied by $\\bf{A}$ produce scalar multiples of themselves.\n",
                "$$\\bf{A}v = \\lambda v$$\n",
                "$\\lambda$, the scaling factor, is known as the **eigenvalue**. We add the additional constrained that eigenvectors have unit length."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "![img](img/eigenvectors.gif)\n",
                "\n",
                "**Q:** Is the yellow vector an eigenvector?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Note:** PCA requires that the data matrix be **centered** (i.e., mean = 0). This is a natural step in calculating the covariance matrix. But we will go one step further in our examples and **standardize** our data (mean = 0 *and* stdev = 1). This helps prevent the directions of maximum variance from being influenced by the scale of our predictors. "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's standardize manually $$\\frac{X-\\mu_X}{\\sigma_X}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_std = ((X-X.mean(axis=0))/X.std())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_std.describe()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we calculate the covaraince between predictors. There are several ways to do this with numpy.\\\n",
                "**Note:** Again, if our standardized data matrix $\\bf{\\tilde{X}}$ is $n\\times p$ ($n$ observations and $p$ predictors) then we want the $p \\times p$ correlation matrix which would be $\\frac{1}{n-1}\\bf{\\tilde{X}^\\top} \\bf{\\tilde{X}}$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "N = X_std.shape[0]\n",
                "\n",
                "X_cov = np.dot(X_std.T, X_std) / (N-1)\n",
                "\n",
                "X_cov2 = (X_std.T@X_std)/(N-1)\n",
                "\n",
                "X_cov3 = np.cov(X_std.T)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "assert np.allclose(X_cov, X_cov2)\n",
                "assert np.allclose(X_cov2, X_cov3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_cov"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can then use `np.linalg.eig` to find the eigenvalues and eigenvectors of the covariance matrix."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "eig_vals, eig_vecs = np.linalg.eig(X_cov)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It is the **columns** of the 2nd object returned which are the eigenvectors."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The numpy `eig` method doesn't guarantee these to be sorted by eigenvalue!\\\n",
                "We need to insure that ourselves."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# get order of eigen values (decreasing)\n",
                "idx = np.argsort(eig_vals)[::-1]\n",
                "# resort eigenvals on this order\n",
                "eig_vals = eig_vals[idx]\n",
                "# resort the eigenvectors (columns)\n",
                "eig_vecs = eig_vecs[:,idx]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "eig_vecs.shape"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here's the eigenvector with the largest eigenvalue (1st column)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "eig_vecs[:,0]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q:** Are we able to put any interpretation on this eigenvector?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### (2) PCA with SVD"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You could also use singular value decomposition (SVD) of the data matrix to get the eigenvectors.\\\n",
                "$\\bf{\\tilde{X}}$ denotes the standardized data matrix.\n",
                "$$\\bf{\\tilde{X}} = \\bf{U\\Sigma V^\\top}$$\n",
                "*Columns* of $\\bf{U}$ are eigenvectors of the *row* (observation) covaraince matrix.\\\n",
                "*Rows* of $\\bf{V^\\top}$ are eigenvectors of the *column* (predictor) covariance matrix.\\\n",
                "It is the later that we are interested in here. You can always check the shapes if you aren't sure.\n",
                "\n",
                "Unlike, `eig`, the results of `svd` *are* sorted by the singular values (which is equivalent to sorting by the eigenvectors)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "U, Sigma, V_T = np.linalg.svd(X_std)\n",
                "U.shape, Sigma.shape, V_T.shape"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here's the eigenvector with the largest eigenvalue (1st row of $\\bf{V^\\top}$)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "V_T[0]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### (3) PCA with SKLearn"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "SKLearn provides the `PCA` class for not only calculating the principle components (`fit`) but also projecting data into the space spanned by the components (`transform`).\\\n",
                "You can also do both at once with `fit_transform`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_std = StandardScaler().fit_transform(X)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pca = PCA().fit(X_std)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_pca = pca.transform(X_std)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_pca.shape"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can access the number of components with the `n_components_` attribute."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pca.n_components_"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There are the `explained_variance_` and `explained_variance_ratio_` attributes as well which describe how much of the variance is explained by each component."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pca.explained_variance_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pca.explained_variance_ratio_"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally, there the components themselves, which are the *rows* of the `components_` attribute."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pca.components_[0]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "All 3 approachs above will give us the same results for principle components!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# sklearn compared to SVD\n",
                "assert np.allclose(pca.components_[0], V_T[0])\n",
                "# sklearn compared to eigenvector decomposition\n",
                "assert np.allclose(pca.components_[0], eig_vecs[:,0])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Predictors' Contributions to Components\n",
                "\n",
                "If we want to see how much predictor $x_i$ contributs to each we can look at the columns of `pca.components_` (remember, the rows of this are the components themselves!) "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For a few of the predictors, print the predictor name and its contributions to each of the principle components."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "i = 2\n",
                "print(X.columns[i], pca.components_[:,i])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "list(zip(X.columns, pca.components_[0]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "list(zip(X.columns, pca.components_[1]))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now let's **create a plot** that shows:\n",
                "1. A 2D PCA projection of the X data\n",
                "2. Annotated arrows showing the contribution of each predictor to the 2 plotted components\n",
                "3. Colored observations based on the value of their `Results` predictor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10,10))\n",
                "passed = df.Results == 1\n",
                "\n",
                "plt.scatter(X_pca[passed,0], X_pca[passed,1], c='green', alpha=0.3, label=\"passed\")\n",
                "plt.scatter(X_pca[~passed,0], X_pca[~passed,1], c='purple', alpha=0.3, label=\"😟\")\n",
                "# scale up contributions by 2x for improved visibility\n",
                "for i, vec in enumerate(pca.components_.T*2):\n",
                "    plt.arrow(0, 0, vec[0], vec[1], color='k', head_width=0.1, alpha=0.5)\n",
                "    offset = 1.3\n",
                "    plt.annotate(X.columns[i],\n",
                "                 (offset*vec[0], offset*vec[1]), color='k', fontsize=15)\n",
                "plt.legend(fontsize=20)\n",
                "plt.xlabel('1st PCA component', fontsize=20)\n",
                "plt.ylabel('2nd PCA component', fontsize=20);\n",
                "plt.title(\"Test Scores in 2-D PCA Projection\", fontsize=25);"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Can you say anything about the relationship between certain tests scores and students' results?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally, we'll plot the explained variance ratio for each component as a bar plot."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.bar(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_)\n",
                "plt.xticks(ticks=range(0, pca.n_components_), labels=[f'PC{i}' for i in range(1, pca.n_components_+1)]);\n",
                "plt.ylabel('explained variance ratio')\n",
                "plt.xlabel('principle components')\n",
                "plt.title('PCA of Test Dataset');"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Do the first few components dominate the explained variance?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Wine Data (Visualization \u0026 PCR)\n",
                "\n",
                "We'll use the popular wine dataset to look at another example of PCA for visualization as well as a first example of principle component regression (PCR)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "wine_df = pd.read_csv('data/winequality.csv')\n",
                "wine_df.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Peek at the columns and the shape."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "wine_df.columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "wine_df.shape"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Projecting the Wine Data onto the 1st 2 Principle Components\n",
                "\n",
                "The projection into the principle component space when fitting our PCR model happened \"under the hood.\" Let's perform the projection explicitly ourself.\n",
                "\n",
                "**First using SVD**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "wine_std = StandardScaler().fit_transform(wine_df)\n",
                "U, Sigma, V_T = np.linalg.svd(wine_std)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "W = V_T.T"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "W[:,0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "Z = wine_std@W"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Create a scatter plot of the wine data projected onto the first 2 principle components."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.scatter(Z[:,0], Z[:,1])\n",
                "plt.xlabel(\"PC1\")\n",
                "plt.ylabel(\"PC2\")\n",
                "plt.title(\"2D PCA Projection of Wine Data\");"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Now with SKlearn** (both projecting and plotting)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pca = PCA(2).fit(wine_std)\n",
                "wine_pca = pca.transform(wine_std)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pca.components_[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.scatter(wine_pca[:,0], wine_pca[:,1])\n",
                "plt.xlabel(\"PC1\")\n",
                "plt.ylabel(\"PC2\")\n",
                "plt.title(\"2D PCA Projection of Wine Data\");"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Are the results the same? If not, can you explain the difference?\n",
                "\n",
                "Do you see any clustering in the projection? Do you have a possible explanation? How might you test it?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Principal Component Regression (PCR)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We haven't covered classification yet in the course so we'll choose a continuous response variable like `alcohol`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "response = 'alcohol'\n",
                "X = wine_df.drop(response, axis=1)\n",
                "y = wine_df[response]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Train / Test split\\\n",
                "Use 20% for test, stratify on the 'red' column, and use 109 for the random state."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=X['red'], random_state=109)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Scale the data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "scaler = StandardScaler()\n",
                "X_std_train = scaler.fit_transform(X_train)\n",
                "X_std_test = scaler.transform(X_test)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Fit and score a linear regression model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "lm = LinearRegression().fit(X_std_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "lm.score(X_std_train, y_train), lm.score(X_std_test, y_test)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's see if we can get some improvement by adding up to degree 3 polynomial features and interation terms."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "poly = PolynomialFeatures(degree=3, include_bias=False).fit(X_std_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_poly_train = poly.transform(X_std_train)\n",
                "X_poly_test = poly.transform(X_std_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_poly_train.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "lm_poly = LinearRegression().fit(X_poly_train, y_train)\n",
                "lm_poly.score(X_poly_train, y_train), lm_poly.score(X_poly_test, y_test)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We'll now try **PCR**, where we fit a regression model using the principle components as the predictors.\n",
                "\n",
                "The first step will be to **fit the PCA object**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pca = PCA().fit(X_poly_train)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we need to decide: **how many components should we use in our regression model?**\n",
                "\n",
                "Let's try to **'eyeball' it by creating a plot** visualizing the amount of variance explained by each new component and look for a point of diminishing returns (i.e., an \"elbow\" or \"knee\" in the plot)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_to_plot = 50\n",
                "plt.plot(range(1, n_to_plot+1), np.cumsum(pca.explained_variance_ratio_[:n_to_plot]), 'o:')\n",
                "plt.xlabel('n components')\n",
                "plt.ylabel('cumulative explained variance ratio') \n",
                "plt.title('Principle Components of Wine Dataset');"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Pick an eyeballed value for n based on your plot."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "eyeballed_n = 7"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "PCA **transform** (i.e., project) the train and test design matrices, **fit** your model, and **score** on train and test.\n",
                "\n",
                "You can either create a new PCA object specifying `n_components`, or you can just index into the full component matrix when fitting and scoreing.\n",
                "\n",
                "How did you do?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_pca_train = pca.transform(X_poly_train)\n",
                "X_pca_test = pca.transform(X_poly_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pcr = LinearRegression().fit(X_pca_train[:,:eyeballed_n], y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pcr.score(X_pca_train[:, :eyeballed_n], y_train), pcr.score(X_pca_test[:, :eyeballed_n], y_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(u\"\\U0001F4A9\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Can you think of a possible explanation for these results?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## PCR Coefficients in terms of the Original Predictors"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cimg src='img/pcr_coefs.png' width=700px\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Using the slide above, calculate the coefficients for your PCR model in terms of the *original* predictors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "m = eyeballed_n\n",
                "Bz = pcr.coef_\n",
                "Z = X_pca_train\n",
                "W = pca.components_.T[:,:eyeballed_n]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "W.shape, Bz.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pcr.intercept_"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You may also wish to visualize the coefficients (there will be a lot of them!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "Bx = W@Bz\n",
                "plt.figure(figsize=(10,6))\n",
                "plt.bar(range(len(Bx)), Bx)\n",
                "plt.ylabel(r\"$\\beta_x$\")\n",
                "plt.xlabel(\"predictors\\n(original $x$ /w poly \u0026 interaction terms)\");"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You should also confirm that you get the same coefficients in terms of the predictors in $X_{\\text{design}}$ when fitting a PCR model on *all* components as you would get from just fitting a linear regression on your original design matrix."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Fit a polynomial model\n",
                "\n",
                "You'll find making use of the `make_pipeline` function and the `Pipeline` class from SKLearn make this whole section much easier.\n",
                "\n",
                "[sklearn.pipeline.make_pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline)\n",
                "\n",
                "Fit and score your model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "poly_model = make_pipeline(StandardScaler(),\n",
                "                     PolynomialFeatures(degree=3, include_bias=False),\n",
                "                     LinearRegression())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "poly_model.steps"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "poly_model.fit(X_train, y_train)\n",
                "\n",
                "poly_model.score(X_train, y_train), cross_val_score(poly_model, X_train, y_train).mean()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "betas = poly_model.get_params()['linearregression'].coef_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "len(betas)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.bar(x=np.arange(1, len(betas)+1), height=betas)\n",
                "plt.ylabel(r\"$\\beta_z$\")\n",
                "plt.xlabel(\"Principal Component\");"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## RidgeCV\n",
                "\n",
                "Try again using the RidgeCV class. Do your results improve?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "ridge = make_pipeline(StandardScaler(),\n",
                "                      PolynomialFeatures(3, include_bias=False),\n",
                "                      RidgeCV())\n",
                "ridge.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "ridge.score(X_train, y_train), ridge.score(X_test, y_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "cross_val_score(ridge, X_train, y_train).mean()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = ridge.get_params()['ridgecv']\n",
                "# model.cv_values_.mean()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "ridge_betas = ridge.get_params()['ridgecv'].coef_\n",
                "\n",
                "sorted_betas = np.argsort(ridge_betas)[::-1]\n",
                "ridge[1].get_feature_names_out()[sorted_betas[:6]]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Principle Component Regression (PCR)\n",
                "\n",
                "Now we'll try PCR. But we'll have to decide on a choice for the number of components."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "d = 3\n",
                "pca_pipe = make_pipeline(StandardScaler(),\n",
                "                         PolynomialFeatures(degree=d),\n",
                "                         PCA())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_pca_train = pca_pipe.fit_transform(X_train)\n",
                "X_pca_test = pca_pipe.transform(X_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_pca_train.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pca = pca_pipe.fit(X_train).get_params()['pca']\n",
                "pca.n_components_"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## PCR, choose n_components that explain 95% of variance\n",
                "\n",
                "You can do this entirely numerically, but we should make a plot too to illustrate our approach."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_thresh = np.argmax([np.cumsum(pca.explained_variance_ratio_) \u003e= 0.95])+1\n",
                "plt.plot(range(1, pca.n_components_+1), np.cumsum(pca.explained_variance_ratio_))\n",
                "# plt.xlim(1)\n",
                "plt.axvline(n_thresh, ls='--', c='k', label=f'hueristic n={n_thresh}')\n",
                "plt.axhline(0.95, ls=':', c='r', alpha=0.3, label=f'95% variance explained')\n",
                "plt.xlabel(\"n components\")\n",
                "plt.ylabel(\"cumulative % variance explained\")\n",
                "xticks = list(range(0, pca.n_components_+1, 50)) + [n_thresh]\n",
                "xticks[0] = 1\n",
                "plt.xticks(xticks);\n",
                "plt.legend()\n",
                "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1))\n",
                "plt.title(\"Selecting N Components that Explain 95% of the Variance\");"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once you have your choice of `n`, fit the PCR model and score."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pcr_pipe = make_pipeline(StandardScaler(),\n",
                "                    PolynomialFeatures(d),\n",
                "                    PCA(n_components=n_thresh),\n",
                "                    LinearRegression())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from copy import deepcopy\n",
                "pcr_95var = deepcopy(pcr_pipe)\n",
                "pcr_95var.fit(X_train, y_train)\n",
                "pcr_95var.score(X_train, y_train), pcr_95var.score(X_test, y_test)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cross-validation for choosing n_components\n",
                "\n",
                "Try using cross validation to choose `n`!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/cv_n_components.py\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_n = np.argmin(val_mses)+1\n",
                "best_n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/cv_n_plot.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now fit a model called `pcr_cv` using your selected $n$ and score the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pcr_cv = make_pipeline(StandardScaler(),\n",
                "                    PolynomialFeatures(d),\n",
                "                    PCA(best_n),\n",
                "                    LinearRegression())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pcr_cv.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "pcr_cv.score(X_train, y_train), pcr_cv.score(X_test, y_test)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Lasso to select best PCR components\n",
                "\n",
                "Use a pipeline with LassoCV to select the best components for the model. Select from roughly twice as many components as you found to be ideal through cross-validation.\n",
                "\n",
                "Call your pipeline `lasso_pcr`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/lasso_pipeline.py\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "lasso_pcr.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "lasso_pcr.score(X_train, y_train), lasso_pcr.score(X_test, y_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "lasso_pcr[-1].alpha_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "lasso_pcr_betas = lasso_pcr.get_params()['lassocv'].coef_\n",
                "(np.abs(lasso_pcr_betas)\u003e0).sum()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Just LASSO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "lassocv = make_pipeline(StandardScaler(),\n",
                "                    PolynomialFeatures(degree=3, include_bias=False),\n",
                "                    LassoCV())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "lassocv.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "lassocv.score(X_train, y_train), lassocv.score(X_test, y_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "lasso_betas = lassocv.get_params()['lassocv'].coef_\n",
                "(np.abs(lasso_betas\u003e0)).sum()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparing the models\n",
                "\n",
                "Which model performed the best on using cross validation on the training data? How about on the test?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "models = dict(poly_model=poly_model,\n",
                "              ridge=ridge,\n",
                "              pcr_95var=pcr_95var,\n",
                "              pcr_cv=pcr_cv,\n",
                "              lasso_pcr=lasso_pcr,\n",
                "              lassocv=lassocv)\n",
                "print(\"R^2 Results\\n\")\n",
                "print(f\"\\t\\t\\tTrain\\tTest\")\n",
                "for name, estimator in models.items():\n",
                "    print(f'{name:\u003e15}:\\t{estimator.score(X_train, y_train):.4f}\\t{estimator.score(X_test, y_test):.4f}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## PCA and Image Data"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's look at the code used to generate the Fashion MNIST visualizations from lecture."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "fashion_df = pd.read_csv('data/fashion-mnist_test.csv.zip')\n",
                "\n",
                "X = fashion_df.drop('label', axis=1)\n",
                "y = fashion_df['label']\n",
                "X.shape, y.shape\n",
                "\n",
                "fashion_labels = ['T-shirt/top',\n",
                "                  'Trouser',\n",
                "                  'Pullover',\n",
                "                  'Dress',\n",
                "                  'Coat',\n",
                "                  'Sandal',\n",
                "                  'Shirt',\n",
                "                  'Sneaker',\n",
                "                  'Bag',\n",
                "                  'Ankle boot']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2,5, figsize=(10,4))\n",
                "axes = axes.ravel()\n",
                "for i, label_name in enumerate(fashion_labels):\n",
                "    axes[i].imshow(X.iloc[np.argmax(y==i)].values.reshape(28,28), cmap='gray')\n",
                "    axes[i].set(xticks=[], yticks=[])\n",
                "    axes[i].set_title(label_name)\n",
                "plt.tight_layout()\n",
                "plt.suptitle('Fashion MNIST Dataset', y=1.05, fontsize=20);\n",
                "plt.savefig('img/fashionMNIST.png')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "fashion_PCA = PCA(n_components=2).fit_transform(X)\n",
                "def plot_reduced(X, y, labels, original_data, show_color=True, imgs=False):\n",
                "    plt.figure(figsize=(15,10))\n",
                "    ax = plt.gca()\n",
                "    sample_idx = np.random.choice(range(len(X)), 10000, replace=False)\n",
                "    X = X[sample_idx]\n",
                "    y = y[sample_idx]\n",
                "    for i in np.unique(y):\n",
                "        color = plt.cm.tab10(i) if show_color else 'blue'\n",
                "        ax.scatter(X[y==i,0], X[y==i,1],\n",
                "                    color=color,\n",
                "                    label=labels[i],\n",
                "                    alpha=0.8)\n",
                "        # include example images\n",
                "        if imgs:\n",
                "            example_img = original_data[y==i].values[0].reshape(int(np.sqrt(original_data.shape[1])), -1)\n",
                "            scaling = 2\n",
                "            example_img_big = np.repeat(np.repeat(example_img, scaling, axis=0), scaling, axis=1)\n",
                "            imagebox = OffsetImage(example_img_big, cmap='gray')\n",
                "            ab = AnnotationBbox(imagebox, X[y==i].mean(axis=0), frameon = False)\n",
                "            ax.add_artist(ab)\n",
                "    ax.legend();\n",
                "    return ax"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "ax = plot_reduced(fashion_PCA, y, fashion_labels, X, show_color=True, imgs=True)\n",
                "ax.set_xlabel('1st PCA Component', fontsize=18)\n",
                "ax.set_ylabel('2nd PCA Component', fontsize=18)\n",
                "ax.set_title('2-D PCA Projection of Fashion MNIST Dataset', fontsize=20);"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## t-SNE "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "T-distributed Stochastic Neighbor Embedding t-SNE is a non-linear dimensionality reductionn method that is popular for visualization.\n",
                "\n",
                "Unlike PCA, which can be thought of as preserving *global* structure by maintaining pair-wise distances in the projected space, t-SNE focuses on preserving *local* structure. That is, points which are 'close' in the original space should be close in the target space."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cimg src='img/tsne_loss.png' width=700px\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "t-SNE is just one of many nonlinear dimensionality techniques available in the sklearn `manifold` module."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load the Fashion MNIST data again\n",
                "fashion_df = pd.read_csv('data/fashion-mnist_test.csv.zip')\n",
                "\n",
                "X = fashion_df.drop('label', axis=1)\n",
                "y = fashion_df['label']\n",
                "X.shape, y.shape\n",
                "\n",
                "fashion_labels = ['T-shirt/top',\n",
                "                  'Trouser',\n",
                "                  'Pullover',\n",
                "                  'Dress',\n",
                "                  'Coat',\n",
                "                  'Sandal',\n",
                "                  'Shirt',\n",
                "                  'Sneaker',\n",
                "                  'Bag',\n",
                "                  'Ankle boot']"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It is important to read the [TSNE documentation](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). There are a lot of hyperparameters!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "fashion_tsne = TSNE(n_components=2,\n",
                "                     init='random',\n",
                "                     learning_rate='auto',\n",
                "                     n_iter=2000,\n",
                "                     n_iter_without_progress=900,\n",
                "                     perplexity=50,\n",
                "                     random_state=109).fit_transform(X)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "ax = plot_reduced(fashion_tsne, y, fashion_labels, X)\n",
                "ax.set_xticks([])\n",
                "ax.set_yticks([])\n",
                "ax.set_title('2-D t-SNE Mapping of the Fashion MNIST Dataset', fontsize=20);\n",
                "plt.savefig('img/FashionMNIST_tsne.png')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cimg src='img/tsne_cons.png' width=700px\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "How do your results change as we tweak the hyperparameters?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        }
    ]
}
