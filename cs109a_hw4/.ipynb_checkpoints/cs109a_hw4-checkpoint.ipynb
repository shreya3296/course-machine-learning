{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b680c7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"cs109a_hw4.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0717bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science\n",
    "\n",
    "## Homework 4: Missing Data & PCA\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2023**<br/>\n",
    "**Instructors**: Pavlos Protopapas & Kevin Rader\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2021-CS109A/master/\"\n",
    "    \"themes/static/css/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e34cc-1d70-45d5-96a5-337abe6f028c",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- Plots should be legible and interpretable without having to refer to the code that generated them, including labels for the $x$- and $y$-axes as well as a descriptive title and/or legend when appropriate.\n",
    "- When asked to interpret a visualization, do not simply describe it (e.g., \"the curve has a steep slope up\"), but instead explain what you think the plot *means*.\n",
    "- The use of 'hard-coded' values to try and pass tests rather than solving problems programmatically will not receive credit.\n",
    "- The use of *extremely* inefficient or error-prone code (e.g., copy-pasting nearly identical commands rather than looping) may result in only partial credit.\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports cell provided below. Please get course staff approval before importing any additional 3rd party libraries.\n",
    "- Enable scrolling output on cells with very long output.\n",
    "- Feel free to add additional code or markdown cells as needed.\n",
    "- Ensure your code runs top to bottom without error and passes all tests by restarting the kernel and running all cells. This is how the notebook will be evaluated (note that this can take a few minutes). \n",
    "- **You should do a \"Restart Kernel and Run All Cells\" before submitting to ensure (1) your notebook actually runs and (2) all output is visible**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab019c4",
   "metadata": {
    "cell_id": "3c67b69c-c35a-45ee-88ff-99c701edb0a0",
    "colab_type": "text",
    "id": "BlViDCbxVtbG"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2020ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "\n",
    "# Import libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# pandas tricks for better display\n",
    "pd.options.display.max_columns = 50  \n",
    "pd.options.display.max_rows = 500     \n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.precision = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02fd250",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "\n",
    "## Notebook contents\n",
    "\n",
    "- [**PART 1 [45 pts]: Principal Componant Analysis**](#part2)\n",
    "  - [Question 1: PCA for Regression [35 pts]](#part1q1)\n",
    "      - [Solutions](#part1q1solution)\n",
    "  - [Question 2: Visualizing Transformed Data [10 pts]](#part1q2)\n",
    "      - [Solutions](#part1q2solution)\n",
    "      \n",
    "- [**PART 2 [55 pts]: Predicting the selling price of used cars (missing data)**](#part1)\n",
    "  - [Overview and Data Description](#part1intro)\n",
    "  - [Question 3: Visualizing Missing Data [10 pts]](#part2q3)\n",
    "      - [Solutions](#part2q3solution)\n",
    "  - [Question 4: Imputation Methods [45 pts]](#part2q4)\n",
    "      - [Solutions](#part2q4solution)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2802b8df",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "    \n",
    "# PART 1 [45 pts]: Principal Component Analysis\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccb6dbe",
   "metadata": {},
   "source": [
    "<a id=\"part2q3\"></a>\n",
    "\n",
    "## <div class='exercise'><b> Question 1: PCA for Regression [35 pts]</b></div>\n",
    "\n",
    "[Return to contents](#contents)\n",
    " \n",
    "\n",
    "In this question, we will be using a dataset called \"Communities and Crime\" adapted from [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime). The modified dataset contains 122 predictor variables and 1 response variable. All numeric data was normalized into the decimal range 0.00-1.00. Some of the predictor variables are:\n",
    "\n",
    "- `householdsize`: mean people per household\n",
    "- `medIncome`: median household income\n",
    "- `PctHousOccup`: percent of housing occupied\n",
    "- `RentMedian`: rental housing - median rent\n",
    "- `PolicReqPerOffic`: total requests for police per police officer\n",
    "\n",
    "And the response variable is \n",
    "\n",
    "- `ViolentCrimesPerPop`: total number of violent crimes per 100K popuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a778b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df = pd.read_csv(\"data/communities_and_crime.csv\", index_col = 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87397b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate predictors and response\n",
    "X, y = df.drop(columns=['ViolentCrimesPerPop']), df['ViolentCrimesPerPop']\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d31b77",
   "metadata": {},
   "source": [
    "<a id=\"part2q3solution\"></a>\n",
    "## Question 1: Solutions \n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c395609c-7c49-4581-bd11-242d7d4d072f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.1** Compute the correlation matrix for the predictor variables in the training data (DO NOT print the entire matrix). Which pairs of distinct predictor variables have correlation greater than 0.99 or less than -0.99? \n",
    "    \n",
    "Store these pairs in a dictionary called `high_corr` where the keys are tuples corresponding to the names of the pair of predictors and the values are the correlation between each pair.\n",
    "\n",
    "**Hint:** A simple method for finding the correlation matrix was demonstrated in the lab on PCA.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f20313e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "...\n",
    "high_corr = ...\n",
    "print(\"The following pairs of predictor variables have correlation greater than 0.99 or less than -0.99:\")\n",
    "display(high_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58aae96",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f195a20f-9040-4ead-ad15-dc760d62d0e6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**1.2** Fit a linear regression model on the **unscaled** training data **using all available predictors**. Store the train and test $R^2$ scores in `linreg_train_r2` and `linreg_test_r2` respectively. Interpret your results.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022f76d",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c866b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "...\n",
    "linreg_train_r2 = ...\n",
    "linreg_test_r2 = ...\n",
    "print(f\"linear regression train R^2: {linreg_train_r2:.4f}\")\n",
    "print(f\"linear regression test R^2: {linreg_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c18d2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943870b2-a65f-429d-86fd-1170d45a003b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "\n",
    "**1.3.1** Standardize both **X_train** and **X_test**, *fitting the scaler on all the data*, and for each number of components $k$ in $k \\in \\{1,2,3,4,5,6,8,10,12,15,20\\}$: \n",
    "\n",
    "  - Fit the PCA transformation with n_components = $k$ on the standardized **X_train**.\n",
    "    \n",
    "  - Apply the PCA transformation to the standardized **X_train**.\n",
    "    \n",
    "  - Use scikit-learn's cross_validate(...) to perform a 10-fold cross validation for a linear regression model on the transformed training data. \n",
    "    \n",
    "  Plot the mean validation MSE for each $k$. Store the best $k$ based on the mean validation MSE as `best_k`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b017a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "...\n",
    "best_k = ...\n",
    "print(f\"The best k is {best_k}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefdc28d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef5f10-a029-4287-ae69-ad36bd94a701",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "    \n",
    "\n",
    "**1.3.2** Now let's compute the $R^2$ value on the testing data:\n",
    "  - Fit the PCA transformation on the standardized **X_train** with n_components equal to the best $k$ above.\n",
    "  - Apply the PCA transformation to the standardized **X_train** and the standardized **X_test**. \n",
    "  - Fit a linear regression model to the PCA-transformed components. Store the train and test $R^2$ scores in `pcr_train_r2` and `pcr_test_r2` respectively.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2863f3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "...\n",
    "pcr_train_r2 = ...\n",
    "pcr_test_r2 = ...\n",
    "print(f\"PCR train R^2: {pcr_train_r2:.4f}\")\n",
    "print(f\"PCR test R^2: {pcr_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc2285",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3fba5-6729-4df3-9a04-303ef484bc0b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**1.4** Compare the $R^2$ value obtained from **3.2** (original predictors) and **3.3.2** (PCR). Provide an explanation for the observed difference in these results.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44282caf-cd59-46f1-910e-a3634b635af2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Display your results\n",
    "pd.DataFrame([\n",
    "    {'model': 'linear regression (original predictors)',\n",
    "     'train $R^2$': linreg_train_r2,\n",
    "     'test $R^2$': linreg_test_r2},\n",
    "    {'model': f'PCR (k={best_k})',\n",
    "     'train $R^2$': pcr_train_r2,\n",
    "     'test $R^2$': pcr_test_r2},\n",
    "]).set_index('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb08ca",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e4fb7f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<a id=\"part1q2\"></a>\n",
    "\n",
    "## <div class='exercise'><b> Question 2: Visualizing Transformed Data [10 pts]</b></div>\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "In this question, we will be using a dataset called \"Better Life Index\" adapted from [Organisation for Economic Co-operation and Development](https://stats.oecd.org/). The modified dataset contains 24 numerical variables and 1 categorical variable. The categorical variable `Country` is the name of the country. Some of the numerical variables include:\n",
    "\n",
    "- `Dwellings without basic facilities`\n",
    "- `Housing expenditure`\n",
    "- `Rooms per person`\n",
    "- `Household net adjusted disposable income`\n",
    "- `Household net financial wealth`\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869ad79-5972-4323-9b44-83d33716e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the datasets\n",
    "df = pd.read_csv(\"data/OECD_well-being.csv\", index_col = 0)\n",
    "print('df shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c57d03-304f-42ae-a21b-3cafb3ca0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate predictors and response\n",
    "country, X = df['Country'], df.drop(columns='Country').values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca7fe1-bb14-4b46-a73a-6040fc2a1bbe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.1** Standardize **X** and apply a PCA transformation with n_components = 2 to your standardized data. Save the transformed data as `X_transformed`.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd9981f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "X_transformed = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad0c31f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b0e18e-65cd-49bc-8111-489b0f7ae44a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**2.2** Make a scatter plot for the transformed data, where the x-axis corresponds to the first principal component, and the y-axis corresponds to the second principal component. The plot should state the amount of variance explained by each component. \n",
    "\n",
    "Label each point by its corresponding country name. Do you observe any pattern in the scatter plot? Be specific and explain.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe5a2fd",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e080d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3eb574-7d3c-48d4-b6fc-8e12aa7e66d1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**2.3** In question 1, where we also used PCA, we had a training and a test set. In question 2 we did not split the data. Explain why.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61356287",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d437b8-56c1-4385-a0e0-2e084fcd5da7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<a id=\"part2\"></a>\n",
    "    \n",
    "# PART 2 [55 pts]: Predicting the selling price of used cars (missing data)\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7405dafa-2007-4570-89cd-1925ff71b818",
   "metadata": {},
   "source": [
    "<a id=\"part1intro\"></a>\n",
    "\n",
    "## Overview and Data Description \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "In this part, we analyze the data about used cars from a [Kaggle project](https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho). The dataset is pre-processed and modified so that it contains missing values. The goal is to handle missing data and predict selling prices from the other features available in this dataset.\n",
    "\n",
    "### Dataset \n",
    "\n",
    "The training dataset is available as `data/vehicle_dataset_train.csv`. It contains the following columns:\n",
    "\n",
    "- `year` - year of the car when it was bought, \n",
    "- `mileage` - mileage of the car,\n",
    "- `max_power` - maximum power of the engine (in bhps),\n",
    "- `selling_price` - price at which the car is being sold (in lakh rupees)\n",
    "\n",
    "The testing dataset is available as `data/vehicle_dataset_test.csv`. It contains all columns mentioned above.\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "We will handle missing data and predict `selling_price` from the other features available in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592bc139-1bbb-41c1-9dbf-9d025894d469",
   "metadata": {},
   "source": [
    "<a id=\"part1q1\"></a>\n",
    "\n",
    "## <div class='exercise'><b>Question 3: Visualizing Missing Data [10 pts]</b></div>\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "**PLEASE NOTE:** In this course, you will be expected to ALWAYS label your axes, title your graphs, and produce visuals which clearly communicate the data (as described in the [Instructions](#instructions) at the start of this notebook). Visuals should often be accompanied by text identifying the key point of the visual and defending any choices you make as a data scientist regarding the visual to best communicate your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf204986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df_train = pd.read_csv(\"data/vehicle_dataset_train.csv\", index_col=0)\n",
    "df_test = pd.read_csv(\"data/vehicle_dataset_test.csv\", index_col=0)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8657eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate predictors from responsess\n",
    "X_train, y_train = df_train.drop(columns=['selling_price']), df_train['selling_price']\n",
    "X_test, y_test = df_test.drop(columns=['selling_price']), df_test['selling_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ed18b-098b-4e51-9714-358381941bbe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**Q3.1** Let's explore the extent of the missingness in the train data:\n",
    "\n",
    "- store the number of rows with missing values in `n_rows_with_missingness`\n",
    "- store the number of columns with missing values in `n_columns_with_missingness`\n",
    "- Create a Pandas Series where the indices are the column names of `X_train` and the values are the number of missing data entries in the corresponding column in `X_train`. Store this series in `col_missingness`\n",
    "\n",
    "**Hint:** The Pandas `isna()` method is very helpful. Keep in mind that, when doing arithmetic with boolean values, `True` evaluates to `1` and `False` evaluates to `0`. With this knowledge, how might you use methods like `sum()` and `max()` along specific axes of a data matrix to answer questions about missingness?\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf85876-dc85-47cf-b489-50a5604837ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_rows_with_missingness = ...\n",
    "n_cols_with_missingness = ...\n",
    "col_missingness = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a45d86-754d-433c-add4-7bead6d6eb3f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# display your results with this code\n",
    "print('# rows with missingness:', n_rows_with_missingness)\n",
    "print('# columns with missingness:', n_cols_with_missingness)\n",
    "print(f'\\ncolumns with missingness:\\n{col_missingness}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d2b8e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baac463-e4d8-4480-8962-68b290428de5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**3.2** Generate a boxplot of `year` for all samples that have missing values. In the same plot, generate another boxplot of `year` for all samples that do not have missing values. Do you see any pattern?  If so, what might be the implications of that pattern? \n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321a8fe3",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609fa5bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e395274-c3ad-4646-b0cd-f29b0c915bc3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<a id=\"part1q2\"></a>\n",
    "\n",
    "## <div class='exercise'><b> Question 4:   Imputation Methods [45 pts]</b></div>\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "We will try different ways of dealing with missing data. Take care not to overwrite the original `X_train` and `X_test` as we'll want to use them each time we try a new imputation method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8cc691-6071-4349-8a55-13428aad1437",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "    \n",
    "\n",
    "**4.1** First, we consider mean imputation:\n",
    "  - Use SimpleImputer to impute the mean of observed `max_power` values in the training dataset for both **X_train** and **X_test**. \n",
    "  - Fit a linear regression model and store its $R^2$ score on the test data in `linreg_meanimp_r2`\n",
    "  - Fit a k-NN regression model ($k$=2) store its $R^2$ score on the test data in `knn_meanimp_r2` \n",
    "    \n",
    "**NOTE:** For the sake of consistency, we will used standardized data throughout question 2 (consider why this is necessary for at least some of our models and imputation methods). Note that we are fitting our scaler on *all* the data (train + test).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5337d-edfa-4905-8520-8c892fce88b0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(np.concatenate([X_train.values, X_test.values], axis=0))\n",
    "X_train_scaled = scaler.transform(X_train.values)\n",
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "# Add back column names lost during scaling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691fb5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f70a6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193128fd-08e1-4a3e-9af9-fe6fb2c0ea5f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.2** Now, we will impute the data using k-NN regression model and see how it works:\n",
    "  - Use KNNImputer ($k$=2) to impute both **X_train** and **X_test**.\n",
    "  - Fit a linear regression model and store its $R^2$ score on the test data in `linreg_knnimp_r2`\n",
    "  - Fit a k-NN regression model ($k$=2) store its $R^2$ score on the test data in `knn_knnimp_r2` \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007f059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea64aaa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a06fd1d-8099-497b-b07a-b931835ed4ff",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "    \n",
    "**4.3** Now, let's examine the indicator method:\n",
    "  - For both the training and testing data, create an additional predictor called `has_missing_value` that indicates whether each row has any missing value.\n",
    "  - Impute the mean of observed `max_power` values in the training dataset for both **X_train** and **X_test**.\n",
    "  - Fit a linear regression model and store its $R^2$ score on the test data in `linreg_indic_r2`\n",
    "  - Fit a k-NN regression model ($k$=2) store its $R^2$ score on the test data in `knn_indic_r2` \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2adb9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b9b711",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e77b2-ff77-4063-b83d-9cf6cb4a69aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**4.4** Compare the $R^2$ values from **4.1 - 4.3**. Does adding an indicator variable help? Do these indicator method results provide any support **for** or **against** a claim that the data is missing completely at random? Why or why not?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a8b37-95bf-4216-9931-80dd37fdba80",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Display your results\n",
    "pd.DataFrame([\n",
    "    {'missingness approach': 'mean imputation',\n",
    "     'linreg test $R^2$': linreg_meanimp_r2,\n",
    "     'knn test $R^2$': knn_meanimp_r2},\n",
    "    {'missingness approach': 'knn imputation',\n",
    "     'linreg test $R^2$': linreg_knnimp_r2,\n",
    "     'knn test $R^2$': knn_knnimp_r2},\n",
    "    {'missingness approach': 'missingness indicator + mean imputation',\n",
    "     'linreg test $R^2$': linreg_indic_r2,\n",
    "     'knn test $R^2$': knn_indic_r2}, \n",
    "]).set_index('missingness approach')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9828cf1",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe67dd5-d713-4e22-9eed-f63a1cd4f848",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "**This concludes HW4. Thank you!**\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1.1": {
     "name": "q1.1",
     "points": 6,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert type(high_corr) == dict and len(high_corr) == 9,\\\n... \"high_corr should be a dict with 9 entries\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> assert all([isinstance(k, tuple) and len(k) == 2 and isinstance(k[0], str) and isinstance(k[1], str) for k in high_corr.keys()]),\\\n...     \"All keys in high_corr should be tuples of two strings.\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> assert all([isinstance(v, (float, np.float_)) and v >= -1 and v <= 1 for v in high_corr.values()]),\\\n...         \"All values in high_corr should be floats between -1 and 1 (inclusive).\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> \n>>> # HIDDEN\n>>> def test_high_corr():\n...     # Step 1: Use a fixed dataset to compute the expected high_corr dictionary.\n...     # (For simplicity, I'm using the same logic as the solution here. In practice, you might use a smaller fixed dataset.)\n...     col_names_test = X_train.columns\n...     df_corr_test = X_train.corr()\n...     itemindex_test = np.where((df_corr_test > 0.99) | (df_corr_test < -0.99))\n...     expected_high_corr = {(col_names_test[i],col_names_test[j]): df_corr_test.iloc[i,j] for (i,j) in zip(itemindex_test[0], itemindex_test[1]) if i > j}\n...     \n...     # Step 2: Compare the student's high_corr dictionary to the expected dictionary.\n...     assert set(high_corr.keys()) == set(expected_high_corr.keys()), \"The keys in your high_corr dictionary do not match the expected keys.\"\n>>> \n>>> test_high_corr()\n",
         "hidden": false,
         "locked": false,
         "points": 3
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.2": {
     "name": "q1.2",
     "points": 6,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert linreg_train_r2 > 0.1 and linreg_train_r2 < 0.9,\\\n... \"The true linreg_train_r2 is between 0.1 and 0.9\"\n",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> # HIDDEN\n>>> assert round(linreg_train_r2, 3) == 0.846 ,\\\n... \"The true linreg_train_r2 is ~0.846\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> assert linreg_test_r2 > 0.1 and linreg_test_r2 < 0.9,\\\n... \"The true linreg_test_r2 is between 0.1 and 0.9\"\n",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> assert round(linreg_test_r2, 3) == 0.219 ,\\\n... \"The true linreg_test_r2 is ~0.219\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.3.1": {
     "name": "q1.3.1",
     "points": 14,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert best_k > 4 and best_k < 15,\\\n... \"The true best_k is between 4 and 15\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> # HIDDEN\n>>> assert best_k == 8 ,\\\n... \"The true best_k is 8\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.3.2": {
     "name": "q1.3.2",
     "points": 6,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert pcr_train_r2 > 0.2 and pcr_train_r2 < 0.8,\\\n... \"The true pcr_train_r2 is between 0.2 and 0.8\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> # HIDDEN\n>>> assert round(pcr_train_r2, 3) == 0.638 ,\\\n... \"The true pcr_train_r2 is ~0.638\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> assert pcr_test_r2 > 0.2 and pcr_test_r2 < 0.8,\\\n... \"The true pcr_test_r2 is between 0.2 and 0.8\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> # HIDDEN\n>>> assert round(pcr_test_r2, 3) == 0.686 ,\\\n... \"The true pcr_test_r2 is ~0.686\"\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.1": {
     "name": "q2.1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert X_transformed.shape == (38, 2),\\\n... \"The transformed data should have shape (38, 2).\"\n",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> assert np.isclose(X_transformed.mean(), 0),\\\n... \"The mean of the transformed data should be 0 (or very close given machine precision).\"\n",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> # HIDDEN\n>>> def test_transformed():\n...     df = pd.read_csv(\"data/OECD_well-being.csv\", index_col = 0)\n...     country, X = df['Country'], df.drop(columns='Country').values\n...     scaler = StandardScaler()\n...     X_std= scaler.fit_transform(X)\n...     pca = PCA(n_components=2)\n...     X_transformed_sol = pca.fit_transform(X_std)\n...     assert np.allclose(X_transformed, X_transformed_sol),\\\n...     \"Transformed data is incorrect. It doesn't match the results in the solutions.\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.1": {
     "name": "q3.1",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def test_n_rows_with_missingness():\n...     assert isinstance(n_rows_with_missingness, (int, np.integer)), \"n_rows_with_missingness should be an integer\" \n...     assert n_rows_with_missingness >=0, \"n_rows_with_missingness should be non-negative\" \n>>> test_n_rows_with_missingness()\n",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> # HIDDEN\n>>> assert n_rows_with_missingness == X_train.isna().max(axis=1).sum(),\\\n... \"n_rows_with_missingness is incorrect\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> def test_n_cols_with_missingness():\n...     assert isinstance(n_cols_with_missingness, (int, np.integer)), \"n_cols_with_missingness should be an integer\" \n...     assert n_cols_with_missingness >=0, \"n_cols_with_missingness should be non-negative\" \n>>> test_n_cols_with_missingness()\n",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> # HIDDEN\n>>> assert n_cols_with_missingness == X_train.isna().max(axis=0).sum(),\\\n... \"n_cols_with_missingness is incorrect\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> def test_col_missingness():\n...     col_missingness_sol = X_train.isna().sum(axis=0)\n...     assert type(col_missingness) == pd.Series, \"col_missingness should be a Pandas Series object\"\n...     assert all([type(i) == str for i in col_missingness.index]), \"Indices of col_missingness should be strings (i.e., the column names)\"\n...     assert set(col_missingness.index) == set(X_train.columns), \"Indices of col_missingness should be the names of the X_train columns.\"\n...     assert all([np.issubdtype(v, np.integer) for v in col_missingness.values]), \"Values in col_missingness should be some type of integer\"\n>>> test_col_missingness()\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> # HIDDEN\n>>> col_missingness_sol = X_train.isna().sum(axis=0)\n>>> assert pd.Series.equals(col_missingness[X_train.columns], col_missingness_sol),\\\n... \"You have the wrong numbers of missing values for the entries in your Series\"\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.1": {
     "name": "q4.1",
     "points": 12,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert linreg_meanimp_r2 > 0.1 and linreg_meanimp_r2 < 0.2,\\\n... \"The true linreg_meanimp_r2 is between 0.1 and 0.2\"\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> # HIDDEN\n>>> assert round(linreg_meanimp_r2, 3) == 0.189 ,\\\n... \"The true linreg_meanimp_r2 is ~0.189\"\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> assert knn_meanimp_r2 > 0.35 and knn_meanimp_r2 < 0.45,\\\n... \"The true knn_meanimp_r2 is between 0.35 and 0.45\"\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> # HIDDEN\n>>> assert round(knn_meanimp_r2, 3) == 0.401 ,\\\n... \"The true knn_meanimp_r2 is between 0.401\"\n",
         "hidden": false,
         "locked": false,
         "points": 3
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.2": {
     "name": "q4.2",
     "points": 12,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert linreg_knnimp_r2 > 0.2 and linreg_knnimp_r2 < 0.3,\\\n... \"The true linreg_knnimp_r2 is between 0.2 and 0.3\"\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> # HIDDEN\n>>> assert round(linreg_knnimp_r2, 3) == 0.205 ,\\\n... \"The true linreg_knnimp_r2 is ~0.205\"\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> assert knn_knnimp_r2 > 0.55 and knn_knnimp_r2 < 0.65,\\\n... \"The true knn_knnimp_r2 is between 0.55 and 0.65\"\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> # HIDDEN\n>>> assert round(knn_knnimp_r2, 3) == 0.622 ,\\\n... \"The true knn_knnimp_r2 is between 0.622\"\n",
         "hidden": false,
         "locked": false,
         "points": 3
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.3": {
     "name": "q4.3",
     "points": 12,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert linreg_indic_r2 > 0.2 and linreg_indic_r2 < 0.5,\\\n... \"The true linreg_indic_r2 is between 0.2 and 0.5\"\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> # HIDDEN\n>>> assert round(linreg_indic_r2, 3) == 0.395 ,\\\n... \"The true linreg_knnimp_r2 is ~0.395\"\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> assert knn_indic_r2 > 0.55 and knn_indic_r2 < 0.75,\\\n... \"The true knn_indic_r2 is between 0.55 and 0.75\"\n",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> # HIDDEN\n>>> assert round(knn_indic_r2, 3) == 0.653 ,\\\n... \"The true knn_indic_r2 is between ~0.653\"\n",
         "hidden": false,
         "locked": false,
         "points": 3
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
